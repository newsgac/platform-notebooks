{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsgac.pipelines.models import Pipeline\n",
    "from newsgac.ace.models import ACE\n",
    "from bson import ObjectId\n",
    "from newsgac.genres import genre_labels\n",
    "from copy import deepcopy\n",
    "from scipy.sparse import csr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ace = ACE.objects.get({'_id': ObjectId('5cffd4fdcd339507a26c475d')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(result=Result(std=0.44406076489546104, recall_macro=0.6663291769231762, precision_macro=0.7526055407318805, fmeasure_weighted=0.7219809327413728, fmeasure_macro=0.693289337083468, precision_weighted=0.7414337668914966, sorted_labels=[0, 7, 8], precision_micro=0.7298043452166607, cohens_kappa=0.5275018534001781, recall_micro=0.7298043452166607, accuracy=0.7298043452166607, confusion_matrix=<newsgac.common.fields.WrappedObject object at 0x7f5ae526a110>, recall_weighted=0.7298043452166607, fmeasure_micro=0.7298043452166607), updated=datetime.datetime(2019, 6, 11, 14, 4, 47, 544000), task=TrackedTask(status=<Status.SUCCESS: 'SUCCESS'>, start=datetime.datetime(2019, 6, 11, 13, 55, 29, 998000), end=datetime.datetime(2019, 6, 11, 14, 4, 47, 544000), task_id=u'146c5f62-fbe4-4c5f-9dc0-c931825a6937'), created=datetime.datetime(2019, 6, 11, 13, 55, 28, 823000), learner=LearnerGB(_tag=u'gb', parameters=Parameters(loss=u'deviance', learning_rate=0.1, n_estimators=100, random_state=42, max_features=u'auto', max_depth=3)), display_title=u'3N 2019 3 GB TFIDF default ', lowercase=False, nlp_tool=TFIDF(_tag=u'tfidf', parameters=Parameters(scaling=True)), data_source=[DataSource id: 5cfa315ce4beae041f728558], user=User(updated=datetime.datetime(2018, 11, 19, 9, 19, 57, 796000), surname=u'TKS', name=u'Erik', created=datetime.datetime(2018, 11, 19, 9, 19, 57, 796000), password=u'$pbkdf2-sha512$25000$NIZwLuWck3IuZcz5P8f4Xw$JSgRlBfXOFWKRjphHOIr645if/2iOvDHKOuv45GGNxEBqXQlk5f2R3vUzUrILKTePvHxEqR073i4.HPxZQq7pQ', email=u'e.tjongkimsang@esciencecenter.nl'), sw_removal=True, _id=ObjectId('5cffb2d0cd339501018b5562'), quote_removal=True, sk_pipeline=<newsgac.common.fields.WrappedObject object at 0x7f5ae2ae1b50>, lemmatization=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ace.pipelines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ace.pipelines[0]\n",
    "article_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = ace.data_source.articles[article_number]\n",
    "sk_pipeline = pipeline.sk_pipeline.get()\n",
    "prediction = sk_pipeline.predict([article.raw_text])[0]\n",
    "\n",
    "# do not modify pipeline.sk_pipeline\n",
    "skp = deepcopy(sk_pipeline)\n",
    "model = skp.steps.pop()[1]\n",
    "\n",
    "used_classes = model.classes_\n",
    "used_class_names = [genre_labels[x] for x in used_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57842"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = skp.named_steps['FeatureExtraction'].get_feature_names()\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "915"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ace.data_source.articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.41 ms, sys: 775 Âµs, total: 8.18 ms\n",
      "Wall time: 660 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prediction = sk_pipeline.predict([article.raw_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions for explaining classifiers that use tabular data (matrices).\n",
    "\"\"\"\n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from lime.discretize import QuartileDiscretizer\n",
    "from lime.discretize import DecileDiscretizer\n",
    "from lime.discretize import EntropyDiscretizer\n",
    "from lime.discretize import BaseDiscretizer\n",
    "from lime import explanation\n",
    "from lime import lime_base\n",
    "\n",
    "\n",
    "class LimeTabularExplainer(object):\n",
    "    \"\"\"Explains predictions on tabular (i.e. matrix) data.\n",
    "    For numerical features, perturb them by sampling from a Normal(0,1) and\n",
    "    doing the inverse operation of mean-centering and scaling, according to the\n",
    "    means and stds in the training data. For categorical features, perturb by\n",
    "    sampling according to the training distribution, and making a binary\n",
    "    feature that is 1 when the value is the same as the instance being\n",
    "    explained.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 training_data,\n",
    "                 mode=\"classification\",\n",
    "                 training_labels=None,\n",
    "                 feature_names=None,\n",
    "                 categorical_features=None,\n",
    "                 categorical_names=None,\n",
    "                 kernel_width=None,\n",
    "                 verbose=False,\n",
    "                 class_names=None,\n",
    "                 feature_selection='auto',\n",
    "                 discretize_continuous=True,\n",
    "                 discretizer='quartile',\n",
    "                 random_state=None):\n",
    "        \"\"\"Init function.\n",
    "\n",
    "        Args:\n",
    "            training_data: numpy 2d array\n",
    "            mode: \"classification\" or \"regression\"\n",
    "            training_labels: labels for training data. Not required, but may be\n",
    "                used by discretizer.\n",
    "            feature_names: list of names (strings) corresponding to the columns\n",
    "                in the training data.\n",
    "            categorical_features: list of indices (ints) corresponding to the\n",
    "                categorical columns. Everything else will be considered\n",
    "                continuous. Values in these columns MUST be integers.\n",
    "            categorical_names: map from int to list of names, where\n",
    "                categorical_names[x][y] represents the name of the yth value of\n",
    "                column x.\n",
    "            kernel_width: kernel width for the exponential kernel.\n",
    "                If None, defaults to sqrt (number of columns) * 0.75\n",
    "            verbose: if true, print local prediction values from linear model\n",
    "            class_names: list of class names, ordered according to whatever the\n",
    "                classifier is using. If not present, class names will be '0',\n",
    "                '1', ...\n",
    "            feature_selection: feature selection method. can be\n",
    "                'forward_selection', 'lasso_path', 'none' or 'auto'.\n",
    "                See function 'explain_instance_with_data' in lime_base.py for\n",
    "                details on what each of the options does.\n",
    "            discretize_continuous: if True, all non-categorical features will\n",
    "                be discretized into quartiles.\n",
    "            discretizer: only matters if discretize_continuous is True. Options\n",
    "                are 'quartile', 'decile', 'entropy' or a BaseDiscretizer\n",
    "                instance.\n",
    "            random_state: an integer or numpy.RandomState that will be used to\n",
    "                generate random numbers. If None, the random state will be\n",
    "                initialized using the internal numpy seed.\n",
    "        \"\"\"\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        self.mode = mode\n",
    "        self.categorical_names = categorical_names or {}\n",
    "\n",
    "        if categorical_features is None:\n",
    "            categorical_features = []\n",
    "        if feature_names is None:\n",
    "            feature_names = [str(i) for i in range(training_data.shape[1])]\n",
    "\n",
    "        self.categorical_features = list(categorical_features)\n",
    "        self.feature_names = list(feature_names)\n",
    "\n",
    "        self.discretizer = None\n",
    "        if discretize_continuous:\n",
    "            if discretizer == 'quartile':\n",
    "                self.discretizer = QuartileDiscretizer(\n",
    "                        training_data, self.categorical_features,\n",
    "                        self.feature_names, labels=training_labels)\n",
    "            elif discretizer == 'decile':\n",
    "                self.discretizer = DecileDiscretizer(\n",
    "                        training_data, self.categorical_features,\n",
    "                        self.feature_names, labels=training_labels)\n",
    "            elif discretizer == 'entropy':\n",
    "                self.discretizer = EntropyDiscretizer(\n",
    "                        training_data, self.categorical_features,\n",
    "                        self.feature_names, labels=training_labels)\n",
    "            elif isinstance(discretizer, BaseDiscretizer):\n",
    "                self.discretizer = discretizer\n",
    "            else:\n",
    "                raise ValueError('''Discretizer must be 'quartile',''' +\n",
    "                                 ''' 'decile', 'entropy' or a''' +\n",
    "                                 ''' BaseDiscretizer instance''')\n",
    "            self.categorical_features = list(range(training_data.shape[1]))\n",
    "            discretized_training_data = self.discretizer.discretize(\n",
    "                    training_data)\n",
    "\n",
    "        if kernel_width is None:\n",
    "            kernel_width = np.sqrt(training_data.shape[1]) * .75\n",
    "        kernel_width = float(kernel_width)\n",
    "\n",
    "        def kernel(d):\n",
    "            return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
    "\n",
    "        self.feature_selection = feature_selection\n",
    "        self.base = lime_base.LimeBase(kernel, verbose, random_state=self.random_state)\n",
    "        self.scaler = None\n",
    "        self.class_names = class_names\n",
    "        self.scaler = sklearn.preprocessing.StandardScaler(with_mean=False)\n",
    "        self.scaler.fit(training_data)\n",
    "        self.feature_values = {}\n",
    "        self.feature_frequencies = {}\n",
    "        print('self.categorical_features len: ', len(self.categorical_features))\n",
    "        print('column len: ', len(training_data[:, self.categorical_features[0]]))\n",
    "\n",
    "#         for feature in self.categorical_features:\n",
    "#             feature_count = collections.defaultdict(lambda: 0.0)\n",
    "#             column = training_data[:, feature]\n",
    "#             if self.discretizer is not None:\n",
    "#                 column = discretized_training_data[:, feature]\n",
    "#                 feature_count[0] = 0.\n",
    "#                 feature_count[1] = 0.\n",
    "#                 feature_count[2] = 0.\n",
    "#                 feature_count[3] = 0.\n",
    "#             for value in column:\n",
    "#                 feature_count[value] += 1\n",
    "#             values, frequencies = map(list, zip(*(feature_count.items())))\n",
    "#             self.feature_values[feature] = values\n",
    "#             self.feature_frequencies[feature] = (np.array(frequencies) /\n",
    "#                                                  sum(frequencies))\n",
    "#             self.scaler.mean_[feature] = 0\n",
    "#             self.scaler.scale_[feature] = 1\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_and_round(values):\n",
    "        return ['%.2f' % v for v in values]\n",
    "\n",
    "    def explain_instance(self,\n",
    "                         data_row,\n",
    "                         predict_fn,\n",
    "                         labels=(1,),\n",
    "                         top_labels=None,\n",
    "                         num_features=10,\n",
    "                         num_samples=5000,\n",
    "                         distance_metric='euclidean',\n",
    "                         model_regressor=None):\n",
    "        \"\"\"Generates explanations for a prediction.\n",
    "\n",
    "        First, we generate neighborhood data by randomly perturbing features\n",
    "        from the instance (see __data_inverse). We then learn locally weighted\n",
    "        linear models on this neighborhood data to explain each of the classes\n",
    "        in an interpretable way (see lime_base.py).\n",
    "\n",
    "        Args:\n",
    "            data_row: 1d numpy array, corresponding to a row\n",
    "            predict_fn: prediction function. For classifiers, this should be a\n",
    "                function that takes a numpy array and outputs prediction\n",
    "                probabilities. For regressors, this takes a numpy array and\n",
    "                returns the predictions. For ScikitClassifiers, this is\n",
    "                `classifier.predict_proba()`. For ScikitRegressors, this\n",
    "                is `regressor.predict()`. The prediction function needs to work\n",
    "                on multiple feature vectors (the vectors randomly perturbed\n",
    "                from the data_row).\n",
    "            labels: iterable with labels to be explained.\n",
    "            top_labels: if not None, ignore labels and produce explanations for\n",
    "                the K labels with highest prediction probabilities, where K is\n",
    "                this parameter.\n",
    "            num_features: maximum number of features present in explanation\n",
    "            num_samples: size of the neighborhood to learn the linear model\n",
    "            distance_metric: the distance metric to use for weights.\n",
    "            model_regressor: sklearn regressor to use in explanation. Defaults\n",
    "                to Ridge regression in LimeBase. Must have model_regressor.coef_\n",
    "                and 'sample_weight' as a parameter to model_regressor.fit()\n",
    "\n",
    "        Returns:\n",
    "            An Explanation object (see explanation.py) with the corresponding\n",
    "            explanations.\n",
    "        \"\"\"\n",
    "        data, inverse = self.__data_inverse(data_row, num_samples)\n",
    "        scaled_data = (data - self.scaler.mean_) / self.scaler.scale_\n",
    "\n",
    "        distances = sklearn.metrics.pairwise_distances(\n",
    "                scaled_data,\n",
    "                scaled_data[0].reshape(1, -1),\n",
    "                metric=distance_metric\n",
    "        ).ravel()\n",
    "\n",
    "        yss = predict_fn(inverse)\n",
    "\n",
    "        # for classification, the model needs to provide a list of tuples - classes\n",
    "        # along with prediction probabilities\n",
    "        if self.mode == \"classification\":\n",
    "            if len(yss.shape) == 1:\n",
    "                raise NotImplementedError(\"LIME does not currently support \"\n",
    "                                          \"classifier models without probability \"\n",
    "                                          \"scores. If this conflicts with your \"\n",
    "                                          \"use case, please let us know: \"\n",
    "                                          \"https://github.com/datascienceinc/lime/issues/16\")\n",
    "            elif len(yss.shape) == 2:\n",
    "                if self.class_names is None:\n",
    "                    self.class_names = [str(x) for x in range(yss[0].shape[0])]\n",
    "                else:\n",
    "                    self.class_names = list(self.class_names)\n",
    "                if not np.allclose(yss.sum(axis=1), 1.0):\n",
    "                    warnings.warn(\"\"\"\n",
    "                    Prediction probabilties do not sum to 1, and\n",
    "                    thus does not constitute a probability space.\n",
    "                    Check that you classifier outputs probabilities\n",
    "                    (Not log probabilities, or actual class predictions).\n",
    "                    \"\"\")\n",
    "            else:\n",
    "                raise ValueError(\"Your model outputs \"\n",
    "                                 \"arrays with {} dimensions\".format(len(yss.shape)))\n",
    "\n",
    "        # for regression, the output should be a one-dimensional array of predictions\n",
    "        else:\n",
    "            try:\n",
    "                assert isinstance(yss, np.ndarray) and len(yss.shape) == 1\n",
    "            except AssertionError:\n",
    "                raise ValueError(\"Your model needs to output single-dimensional \\\n",
    "                    numpyarrays, not arrays of {} dimensions\".format(yss.shape))\n",
    "\n",
    "            predicted_value = yss[0]\n",
    "            min_y = min(yss)\n",
    "            max_y = max(yss)\n",
    "\n",
    "            # add a dimension to be compatible with downstream machinery\n",
    "            yss = yss[:, np.newaxis]\n",
    "\n",
    "        feature_names = copy.deepcopy(self.feature_names)\n",
    "        if feature_names is None:\n",
    "            feature_names = [str(x) for x in range(data_row.shape[0])]\n",
    "\n",
    "        values = self.convert_and_round(data_row)\n",
    "\n",
    "        for i in self.categorical_features:\n",
    "            if self.discretizer is not None and i in self.discretizer.lambdas:\n",
    "                continue\n",
    "            name = int(data_row[i])\n",
    "            if i in self.categorical_names:\n",
    "                name = self.categorical_names[i][name]\n",
    "            feature_names[i] = '%s=%s' % (feature_names[i], name)\n",
    "            values[i] = 'True'\n",
    "        categorical_features = self.categorical_features\n",
    "\n",
    "        discretized_feature_names = None\n",
    "        if self.discretizer is not None:\n",
    "            categorical_features = range(data.shape[1])\n",
    "            discretized_instance = self.discretizer.discretize(data_row)\n",
    "            discretized_feature_names = copy.deepcopy(feature_names)\n",
    "            for f in self.discretizer.names:\n",
    "                discretized_feature_names[f] = self.discretizer.names[f][int(\n",
    "                        discretized_instance[f])]\n",
    "\n",
    "        domain_mapper = TableDomainMapper(feature_names,\n",
    "                                          values,\n",
    "                                          scaled_data[0],\n",
    "                                          categorical_features=categorical_features,\n",
    "                                          discretized_feature_names=discretized_feature_names)\n",
    "        ret_exp = explanation.Explanation(domain_mapper,\n",
    "                                          mode=self.mode,\n",
    "                                          class_names=self.class_names)\n",
    "        ret_exp.scaled_data = scaled_data\n",
    "        if self.mode == \"classification\":\n",
    "            ret_exp.predict_proba = yss[0]\n",
    "            if top_labels:\n",
    "                labels = np.argsort(yss[0])[-top_labels:]\n",
    "                ret_exp.top_labels = list(labels)\n",
    "                ret_exp.top_labels.reverse()\n",
    "        else:\n",
    "            ret_exp.predicted_value = predicted_value\n",
    "            ret_exp.min_value = min_y\n",
    "            ret_exp.max_value = max_y\n",
    "            labels = [0]\n",
    "\n",
    "        for label in labels:\n",
    "            (ret_exp.intercept[label],\n",
    "             ret_exp.local_exp[label],\n",
    "             ret_exp.score, ret_exp.local_pred) = self.base.explain_instance_with_data(\n",
    "                    scaled_data,\n",
    "                    yss,\n",
    "                    distances,\n",
    "                    label,\n",
    "                    num_features,\n",
    "                    model_regressor=model_regressor,\n",
    "                    feature_selection=self.feature_selection)\n",
    "\n",
    "        if self.mode == \"regression\":\n",
    "            ret_exp.intercept[1] = ret_exp.intercept[0]\n",
    "            ret_exp.local_exp[1] = [x for x in ret_exp.local_exp[0]]\n",
    "            ret_exp.local_exp[0] = [(i, -1 * j) for i, j in ret_exp.local_exp[1]]\n",
    "\n",
    "        return ret_exp\n",
    "\n",
    "    def __data_inverse(self,\n",
    "                       data_row,\n",
    "                       num_samples):\n",
    "        \"\"\"Generates a neighborhood around a prediction.\n",
    "\n",
    "        For numerical features, perturb them by sampling from a Normal(0,1) and\n",
    "        doing the inverse operation of mean-centering and scaling, according to\n",
    "        the means and stds in the training data. For categorical features,\n",
    "        perturb by sampling according to the training distribution, and making\n",
    "        a binary feature that is 1 when the value is the same as the instance\n",
    "        being explained.\n",
    "\n",
    "        Args:\n",
    "            data_row: 1d numpy array, corresponding to a row\n",
    "            num_samples: size of the neighborhood to learn the linear model\n",
    "\n",
    "        Returns:\n",
    "            A tuple (data, inverse), where:\n",
    "                data: dense num_samples * K matrix, where categorical features\n",
    "                are encoded with either 0 (not equal to the corresponding value\n",
    "                in data_row) or 1. The first row is the original instance.\n",
    "                inverse: same as data, except the categorical features are not\n",
    "                binary, but categorical (as the original data)\n",
    "        \"\"\"\n",
    "        data = np.zeros((num_samples, data_row.shape[0]))\n",
    "        categorical_features = range(data_row.shape[0])\n",
    "        if self.discretizer is None:\n",
    "            data = self.random_state.normal(\n",
    "                    0, 1, num_samples * data_row.shape[0]).reshape(\n",
    "                    num_samples, data_row.shape[0])\n",
    "            data = data * self.scaler.scale_ + self.scaler.mean_\n",
    "            categorical_features = self.categorical_features\n",
    "            first_row = data_row\n",
    "        else:\n",
    "            first_row = self.discretizer.discretize(data_row)\n",
    "        data[0] = data_row.copy()\n",
    "        inverse = data.copy()\n",
    "        for column in categorical_features:\n",
    "            values = self.feature_values[column]\n",
    "            freqs = self.feature_frequencies[column]\n",
    "            inverse_column = self.random_state.choice(values, size=num_samples,\n",
    "                                                      replace=True, p=freqs)\n",
    "            binary_column = np.array([1 if x == first_row[column]\n",
    "                                      else 0 for x in inverse_column])\n",
    "            binary_column[0] = 1\n",
    "            inverse_column[0] = data[0, column]\n",
    "            data[:, column] = binary_column\n",
    "            inverse[:, column] = inverse_column\n",
    "        if self.discretizer is not None:\n",
    "            inverse[1:] = self.discretizer.undiscretize(inverse[1:])\n",
    "        inverse[0] = data_row\n",
    "        return data, inverse\n",
    "\n",
    "def get_lime_feature_explanation(article, prediction, skp, predict_proba, training_articles, used_class_names):\n",
    "    feature_names = skp.named_steps['FeatureExtraction'].get_feature_names()\n",
    "    print('a')\n",
    "    # calculate feature vectors for explanators\n",
    "    data = skp.transform([a.raw_text for a in training_articles])\n",
    "    # labels = [a.label for a in training_articles]\n",
    "    if isinstance(data, csr_matrix):\n",
    "        data = data.toarray()\n",
    "    print('b')\n",
    "    num_samples = 5000\n",
    "    # In case we have a TFIDF pipeline there are a lot of features\n",
    "    # We should increase num_samples to at least a multitude of the\n",
    "    # amount of features.\n",
    "    if len(feature_names) > 1000:\n",
    "        num_samples = len(feature_names) * 5\n",
    "    \n",
    "    num_samples = 1\n",
    "    print('c')\n",
    "    exp_lime = LimeTabularExplainer(\n",
    "        training_data=data,\n",
    "        feature_names=feature_names,\n",
    "        class_names=used_class_names\n",
    "    )\n",
    "#     print('d')\n",
    "\n",
    "#     feature_list = skp.transform([article.raw_text])\n",
    "#     if isinstance(feature_list, csr_matrix):\n",
    "#         feature_list = feature_list.toarray()\n",
    "#     article_features = feature_list[0]\n",
    "\n",
    "#     print('e')\n",
    "    \n",
    "#     # TODO: check the order of features with feature_names\n",
    "#     return exp_lime.explain_instance(\n",
    "#         article_features,\n",
    "#         predict_proba,\n",
    "#         labels=[prediction],\n",
    "#         num_features=1,\n",
    "#         num_samples=1,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n",
      "('self.categorical_features len: ', 57842)\n",
      "('column len: ', 8331)\n",
      "CPU times: user 1min 12s, sys: 6.22 s, total: 1min 18s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lime_result = get_lime_feature_explanation(\n",
    "    article,\n",
    "    prediction,\n",
    "    skp,\n",
    "    model.predict_proba,\n",
    "    pipeline.data_source.articles,\n",
    "    used_class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481881702"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "57842 * 8331"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
