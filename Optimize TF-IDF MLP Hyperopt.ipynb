{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install imbalanced-learn --user\n",
    "# !{sys.executable} -m pip install hyperopt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/newsgac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from hyperopt import tpe, fmin, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "\n",
    "from newsgac import config\n",
    "from newsgac.genres import genre_codes\n",
    "from newsgac.learners import learners, LearnerSVC, LearnerNB, LearnerXGB, LearnerGB, LearnerMLP, LearnerRF, LearnerLGBM\n",
    "from newsgac.pipelines.get_sk_pipeline import get_sk_pipeline\n",
    "from newsgac.pipelines.utils import report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from newsgac import database\n",
    "from newsgac.data_sources import DataSource\n",
    "from newsgac.pipelines import Pipeline\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'N2BGS Training',\n",
       " u'N2BGS Testing',\n",
       " u'Linked NRC (2930)',\n",
       " u'BOW + 5 features (N2BGS train)',\n",
       " u'BOW + 5 features (N2BGS test)',\n",
       " u'nrc-1950-1994-a.txt',\n",
       " u'nrc-1950-1994-b.txt',\n",
       " u'nrc-1950-1994-c.txt',\n",
       " u'nrc-1950-1994-d.txt',\n",
       " u'nrc-1950-1994-e.txt',\n",
       " u'telegraaf-1950-1994-a.txt',\n",
       " u'telegraaf-1950-1994-b.txt',\n",
       " u'telegraaf-1950-1994-c.txt',\n",
       " u'telegraaf-1950-1994-d.txt',\n",
       " u'telegraaf-1950-1994-e.txt',\n",
       " u'volkskrant-1950-1995-a.txt',\n",
       " u'volkskrant-1950-1995-b.txt',\n",
       " u'volkskrant-1950-1995-c.txt',\n",
       " u'volkskrant-1950-1995-d.txt',\n",
       " u'volkskrant-1950-1995-e.txt',\n",
       " u'nrc-1965.txt',\n",
       " u'nrc-1985.txt',\n",
       " u'BOW + 9 features (N2BGS train)',\n",
       " u'BOW + 9 features (N2BGS test)',\n",
       " u'Linked NRC (2930/9 features)',\n",
       " u'BOW Train unique (9 features)',\n",
       " u'BOW Test unique (9 features)',\n",
       " u'Linked NRC (unique/9 features)',\n",
       " u'BOW Train unique (N3BGS/9 features)',\n",
       " u'N3BGS FROG Test',\n",
       " u'N3BGS FROG Train',\n",
       " u'BOW Test unique (N3BGS/9 features)',\n",
       " u'BOW Test unique (N3BGS/9 features/collapsed)',\n",
       " u'Linked NRC (2930 articles)',\n",
       " u'Combined N3BGS/Linked NRC (3884)',\n",
       " u'2019 N3BGS Test',\n",
       " u'2019 N3BGS Train',\n",
       " u'AAOB',\n",
       " u'20190228 unbalanced train size=3443',\n",
       " u'20190228 unbalanced test size=3443']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d.display_title for d in DataSource.objects.all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'dummy frog',\n",
       " u'dummy bow',\n",
       " u'RF FROG',\n",
       " u'SVC FROG',\n",
       " u'XGB FROG',\n",
       " u'NB FROG',\n",
       " u'MLP FROG',\n",
       " u'RF BOW',\n",
       " u'SVC BOW',\n",
       " u'XGB BOW',\n",
       " u'NB BOW',\n",
       " u'MLP BOW',\n",
       " u'SVC LIN BOW',\n",
       " u'Erik MLP BOW (with stop-words)',\n",
       " u'Erik MLP BOW (5 features) ',\n",
       " u'Erik MLP BOW (9 features) ',\n",
       " u'Erik SVC BOW (9 features)',\n",
       " u'Erik RF BOW (9 features)',\n",
       " u'Erik XGB BOW (9 features)',\n",
       " u'Erik NB BOW (9 features)',\n",
       " u'Erik MLP unique 9 features',\n",
       " u'Erik NB unique (9 features)',\n",
       " u'Erik XGB unique (9 features)',\n",
       " u'Erik SVC unique (9 features)',\n",
       " u'Erik RF unique (9 features)',\n",
       " u'N3BGS MLP unique 9 features',\n",
       " u'N3BGS NB unique 9 features ',\n",
       " u'N3BGS SVC unique 9 features ',\n",
       " u'N3BGS RF unique 9 features',\n",
       " u'N3BGS XGB unique 9 features ',\n",
       " u'dummy n3bgs bow',\n",
       " u'dummy n3bgs frog',\n",
       " u'RF FROG N3BGS',\n",
       " u'SVC FROG N3BGS',\n",
       " u'XGB FROG N3BGS',\n",
       " u'Erik NB N3BGS Collapsed',\n",
       " u'Erik MLP N3BGS Collapsed',\n",
       " u'Erik MLP Linked NRC (2930 articles)',\n",
       " u'Erik RF Linked NRC (2930 articles) ',\n",
       " u'Erik NB Linked NRC (2930 articles) ',\n",
       " u'Erik SVC Linked NRC (2930 articles) ',\n",
       " u'Erik XGB Linked NRC (2930 articles) ',\n",
       " u'SVC BOW Combined',\n",
       " u'NB FROG N3BGS',\n",
       " u'MLP FROG N3BGS',\n",
       " u'RF BOW NBGS',\n",
       " u'SVC BOW N3BGS',\n",
       " u'XGB BOW N3BGS',\n",
       " u'NB BOW N3BGS',\n",
       " u'MLP BOW N3BGS',\n",
       " u'KIM N3BGS FROG and TF-IDF',\n",
       " u'KIM N3BGS MLP Combi features',\n",
       " u'KIM N3BGS RF COMBI',\n",
       " u'KIM N3BGS SVM COMBI (copy)',\n",
       " u'KIM N3BGS XGB COMBI',\n",
       " u'20190228 SVM',\n",
       " u'20190228 SVC no stop words',\n",
       " u'20190228 SVC no stop words, lowercased',\n",
       " u'dummy combi for opt',\n",
       " u'dummy combi for opt ugs',\n",
       " u'new dummy ugs opt',\n",
       " u'new dummy bgs opt',\n",
       " u'dummy opt frog bgs',\n",
       " u'dummy bow opt bgs',\n",
       " u'dummy frog ugs opt',\n",
       " u'dummy bow opt ugs']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d.display_title for d in Pipeline.objects.all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSource: 20190228 unbalanced train size=3443\n",
      "NLP Tool: TF-IDF\n",
      "Classifier: Light GBM\n",
      "Task status: Status.SUCCESS\n"
     ]
    }
   ],
   "source": [
    "p = Pipeline.objects.all()[65]\n",
    "print 'DataSource: ' + p.data_source.display_title\n",
    "print 'NLP Tool: ' + p.nlp_tool.name\n",
    "print 'Classifier: ' + p.learner.name\n",
    "print 'Task status: ' + str(p.task.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3099"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p.data_source.articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data source: 20190228 unbalanced test size=3443\n"
     ]
    }
   ],
   "source": [
    "test_data_source = DataSource.objects.all()[39]\n",
    "print 'Testing data source: ' + test_data_source.display_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CleanOCR', <newsgac.nlp_tools.transformers.CleanOCR at 0x7fda5a3ad390>),\n",
       " ('StopWordRemoval',\n",
       "  <newsgac.nlp_tools.transformers.StopWordRemoval at 0x7fda5a3ad590>),\n",
       " ('FeatureExtraction', FeatureUnion(n_jobs=None,\n",
       "         transformer_list=[('TFIDF', Pipeline(memory=None,\n",
       "       steps=[('RemoveQuotes', <newsgac.nlp_tools.transformers.RemoveQuotes object at 0x7fda5a964510>), ('TF-IDF', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "          dtype=<type 'numpy.float64'>, encoding=u'utf-8', input=u'content',\n",
       "          lowercase=True, max_df=1.0, max_featu...        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "          vocabulary=None))]))],\n",
       "         transformer_weights=None)),\n",
       " ('RobustScaler',\n",
       "  RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=False,\n",
       "         with_scaling=True))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skp = p.get_sk_pipeline()\n",
    "skp_opt = deepcopy(skp)\n",
    "skp_opt.steps.pop()\n",
    "skp_opt.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = np.array([article.raw_text for article in p.data_source.articles])\n",
    "labels = np.array([article.label for article in p.data_source.articles])\n",
    "\n",
    "X = skp_opt.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test dataset for optimization accuracy\n",
    "texts_test = np.array([article.raw_text for article in test_data_source.articles])\n",
    "labels_test = np.array([article.label for article in test_data_source.articles])\n",
    "\n",
    "X_test = skp_opt.transform(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X\n",
    "y_train = labels\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(344,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_layer_sizes_list =[(20, 30), (30, 10), (10, 20), (5,10)]\n",
    "hidden_layer_sizes_list =[(200, 100), (100, 200), (150, 300), (300, 100), (100, 200, 100), (200,100,200)]\n",
    "activation_list = ['identity', 'logistic', 'tanh', 'relu']\n",
    "solver_list = ['lbfgs','sgd', 'adam']\n",
    "learning_rate_list = ['constant','adaptive', 'invscaling']\n",
    "batch_size_list = np.arange(20, 200, dtype=int)\n",
    "max_iter_list = np.arange(20, 500, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(evals, trials, optimizer=tpe.suggest):\n",
    "    space = {\n",
    "        'hidden_layer_sizes': hp.choice('hidden_layer_sizes', hidden_layer_sizes_list),\n",
    "        'activation': hp.choice('activation', activation_list),\n",
    "        'solver': hp.choice('solver', solver_list),\n",
    "        'alpha': hp.lognormal('alpha', 0, 1), # [0.0001, 0.05],\n",
    "#         'alpha': hp.loguniform('alpha', np.log(0.1), np.log(10)),\n",
    "        'learning_rate': hp.choice('learning_rate', learning_rate_list),\n",
    "        'batch_size': hp.choice('batch_size', batch_size_list),\n",
    "        'max_iter': hp.choice('max_iter', max_iter_list),\n",
    "    }\n",
    "    best = fmin(score, space, algo=optimizer, max_evals=evals, trials=trials)\n",
    "    pbar.close()\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_list = []\n",
    "train_loss_list = []\n",
    "test_acc_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "def score(params):\n",
    "     \n",
    "    model = MLPClassifier(\n",
    "            hidden_layer_sizes = params['hidden_layer_sizes'],\n",
    "            activation = params['activation'],\n",
    "            solver = params['solver'],\n",
    "            alpha = params['alpha'],\n",
    "            learning_rate = params['learning_rate'],\n",
    "            batch_size = params['batch_size'],\n",
    "            max_iter = params['max_iter'],\n",
    "            shuffle = True,\n",
    "            random_state = 42,\n",
    "            warm_start = False,\n",
    "            early_stopping = True,\n",
    "            validation_fraction = 0.1,\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)   \n",
    "\n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "    train_loss = log_loss(y_train, model.predict_proba(X_train))\n",
    "    test_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    test_loss = log_loss(y_test, model.predict_proba(X_test))\n",
    "    \n",
    "    train_acc_list.append(train_acc)\n",
    "    train_loss_list.append(train_loss)\n",
    "    test_acc_list.append(test_acc)\n",
    "    test_loss_list.append(test_loss)\n",
    "    \n",
    "    print('Parameters with this training accuracy {} and loss {} :'.format(train_acc, train_loss))\n",
    "    print('Parameters with this testing accuracy {} and loss {} :'.format(test_acc, test_loss))\n",
    "    print(params)\n",
    "    pbar.update()\n",
    "#     return {'loss': test_loss, 'status': STATUS_OK}\n",
    "    return {'loss': -test_acc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s, best loss: ?]\u001b[A\n",
      "\u001b[AParameters with this training accuracy 0.429493384963 and loss 1.95003853758 :\n",
      "\n",
      "  0%|          | 0/1000 [00:51<?, ?it/s, best loss: ?]\u001b[A\n",
      "\u001b[AParameters with this testing accuracy 0.468023255814 and loss 1.90519702132 :\n",
      "\n",
      "  0%|          | 0/1000 [00:51<?, ?it/s, best loss: ?]\u001b[A\n",
      "\u001b[A{'solver': 'sgd', 'hidden_layer_sizes': (200, 100, 200), 'alpha': 1.1171298533861638, 'activation': 'identity', 'max_iter': 474, 'learning_rate': 'constant', 'batch_size': 167}\n",
      "\n",
      "  0%|          | 0/1000 [00:51<?, ?it/s, best loss: ?]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt:   0%|          | 1/1000 [00:52<14:28:47, 52.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/1000 [00:52<14:25:57, 52.01s/it, best loss: -0.468023255814]\u001b[A\n",
      "\u001b[AParameters with this training accuracy 0.780251694095 and loss 0.634446304926 :\n",
      "\n",
      "  0%|          | 1/1000 [03:16<14:25:57, 52.01s/it, best loss: -0.468023255814]\u001b[A\n",
      "\u001b[AParameters with this testing accuracy 0.508720930233 and loss 2.18926604528 :\n",
      "\n",
      "  0%|          | 1/1000 [03:16<14:25:57, 52.01s/it, best loss: -0.468023255814]\u001b[A\n",
      "\u001b[A{'solver': 'lbfgs', 'hidden_layer_sizes': (100, 200, 100), 'alpha': 0.7641217625681462, 'activation': 'logistic', 'max_iter': 290, 'learning_rate': 'constant', 'batch_size': 92}\n",
      "\n",
      "  0%|          | 1/1000 [03:16<14:25:57, 52.01s/it, best loss: -0.468023255814]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt:   0%|          | 2/1000 [03:16<22:09:49, 79.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 2/1000 [03:16<22:07:50, 79.83s/it, best loss: -0.508720930233]\u001b[A\n",
      "\u001b[AParameters with this training accuracy 0.98160696999 and loss 0.0859501290815 :\n",
      "\n",
      "  0%|          | 2/1000 [04:34<22:07:50, 79.83s/it, best loss: -0.508720930233]\u001b[A\n",
      "\u001b[AParameters with this testing accuracy 0.572674418605 and loss 2.3874130397 :\n",
      "\n",
      "  0%|          | 2/1000 [04:34<22:07:50, 79.83s/it, best loss: -0.508720930233]\u001b[A\n",
      "\u001b[A{'solver': 'lbfgs', 'hidden_layer_sizes': (100, 200), 'alpha': 2.5739790754065903, 'activation': 'relu', 'max_iter': 177, 'learning_rate': 'adaptive', 'batch_size': 77}\n",
      "\n",
      "  0%|          | 2/1000 [04:34<22:07:50, 79.83s/it, best loss: -0.508720930233]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt:   0%|          | 3/1000 [04:34<21:57:30, 79.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 3/1000 [04:34<21:56:06, 79.20s/it, best loss: -0.572674418605]\u001b[A\n",
      "\u001b[AParameters with this training accuracy 0.431106808648 and loss 2.48512349238 :\n",
      "\n",
      "  0%|          | 3/1000 [04:44<21:56:06, 79.20s/it, best loss: -0.572674418605]\u001b[A\n",
      "\u001b[AParameters with this testing accuracy 0.468023255814 and loss 2.47390033793 :\n",
      "\n",
      "  0%|          | 3/1000 [04:44<21:56:06, 79.20s/it, best loss: -0.572674418605]\u001b[A\n",
      "\u001b[A{'solver': 'sgd', 'hidden_layer_sizes': (100, 200, 100), 'alpha': 0.19120818768478043, 'activation': 'tanh', 'max_iter': 476, 'learning_rate': 'constant', 'batch_size': 153}\n",
      "\n",
      "  0%|          | 3/1000 [04:44<21:56:06, 79.20s/it, best loss: -0.572674418605]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt:   0%|          | 4/1000 [04:45<16:12:53, 58.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 4/1000 [04:44<16:11:55, 58.55s/it, best loss: -0.572674418605]\u001b[A\n",
      "\u001b[AParameters with this training accuracy 0.436915133914 and loss 1.81051960072 :\n",
      "\n",
      "  0%|          | 4/1000 [06:57<16:11:55, 58.55s/it, best loss: -0.572674418605]\u001b[A\n",
      "\u001b[AParameters with this testing accuracy 0.46511627907 and loss 1.7674944803 :\n",
      "\n",
      "  0%|          | 4/1000 [06:57<16:11:55, 58.55s/it, best loss: -0.572674418605]\u001b[A\n",
      "\u001b[A{'solver': 'sgd', 'hidden_layer_sizes': (200, 100, 200), 'alpha': 1.3698327193232174, 'activation': 'tanh', 'max_iter': 112, 'learning_rate': 'constant', 'batch_size': 81}\n",
      "\n",
      "  0%|          | 4/1000 [06:57<16:11:55, 58.55s/it, best loss: -0.572674418605]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt:   0%|          | 5/1000 [06:57<22:20:23, 80.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 5/1000 [06:57<22:19:43, 80.79s/it, best loss: -0.572674418605]\u001b[A\n",
      "\u001b[AParameters with this training accuracy 0.431106808648 and loss 2.22043046825 :\n",
      "\n",
      "  0%|          | 5/1000 [07:56<22:19:43, 80.79s/it, best loss: -0.572674418605]\u001b[A\n",
      "\u001b[AParameters with this testing accuracy 0.468023255814 and loss 2.18194555482 :\n",
      "\n",
      "  0%|          | 5/1000 [07:56<22:19:43, 80.79s/it, best loss: -0.572674418605]\u001b[A\n",
      "\u001b[A{'solver': 'adam', 'hidden_layer_sizes': (200, 100, 200), 'alpha': 7.084213968043841, 'activation': 'relu', 'max_iter': 453, 'learning_rate': 'invscaling', 'batch_size': 112}\n",
      "\n",
      "  0%|          | 5/1000 [07:56<22:19:43, 80.79s/it, best loss: -0.572674418605]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt:   1%|          | 6/1000 [07:56<20:29:48, 74.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 6/1000 [07:56<20:29:21, 74.21s/it, best loss: -0.572674418605]\u001b[A\n",
      "\u001b[AParameters with this training accuracy 0.430784123911 and loss 2.08885026975 :\n",
      "\n",
      "  1%|          | 6/1000 [11:24<20:29:21, 74.21s/it, best loss: -0.572674418605]\u001b[A\n",
      "\u001b[AParameters with this testing accuracy 0.470930232558 and loss 2.03640622287 :\n",
      "\n",
      "  1%|          | 6/1000 [11:24<20:29:21, 74.21s/it, best loss: -0.572674418605]\u001b[A\n",
      "\u001b[A{'solver': 'sgd', 'hidden_layer_sizes': (300, 100), 'alpha': 0.4877716991162354, 'activation': 'identity', 'max_iter': 362, 'learning_rate': 'invscaling', 'batch_size': 32}\n",
      "\n",
      "  1%|          | 6/1000 [11:24<20:29:21, 74.21s/it, best loss: -0.572674418605]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt:   1%|          | 7/1000 [11:24<31:32:28, 114.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 7/1000 [11:24<31:32:11, 114.33s/it, best loss: -0.572674418605]\u001b[A\n",
      "\u001b[AParameters with this training accuracy 0.429493384963 and loss 1.95143311843 :\n",
      "\n",
      "  1%|          | 7/1000 [14:52<31:32:11, 114.33s/it, best loss: -0.572674418605]\u001b[A\n",
      "\u001b[AParameters with this testing accuracy 0.470930232558 and loss 1.90632541999 :\n",
      "\n",
      "  1%|          | 7/1000 [14:52<31:32:11, 114.33s/it, best loss: -0.572674418605]\u001b[A\n",
      "\u001b[A{'solver': 'sgd', 'hidden_layer_sizes': (200, 100, 200), 'alpha': 1.184029022136145, 'activation': 'identity', 'max_iter': 418, 'learning_rate': 'adaptive', 'batch_size': 149}\n",
      "\n",
      "  1%|          | 7/1000 [14:52<31:32:11, 114.33s/it, best loss: -0.572674418605]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt:   1%|          | 8/1000 [14:52<39:17:25, 142.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 8/1000 [14:52<39:17:10, 142.57s/it, best loss: -0.572674418605]\u001b[A\n",
      "\u001b[AParameters with this training accuracy 0.431106808648 and loss 1.99824680332 :\n",
      "\n",
      "  1%|          | 8/1000 [27:27<39:17:10, 142.57s/it, best loss: -0.572674418605]\u001b[A\n",
      "\u001b[AParameters with this testing accuracy 0.468023255814 and loss 1.9581148206 :\n",
      "\n",
      "  1%|          | 8/1000 [27:27<39:17:10, 142.57s/it, best loss: -0.572674418605]\u001b[A\n",
      "\u001b[A{'solver': 'sgd', 'hidden_layer_sizes': (200, 100, 200), 'alpha': 0.5395456678062278, 'activation': 'logistic', 'max_iter': 186, 'learning_rate': 'adaptive', 'batch_size': 32}\n",
      "\n",
      "  1%|          | 8/1000 [27:27<39:17:10, 142.57s/it, best loss: -0.572674418605]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperopt:   1%|          | 9/1000 [27:27<89:48:42, 326.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 9/1000 [27:27<89:48:32, 326.25s/it, best loss: -0.572674418605]\u001b[A"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "trials = Trials()\n",
    "cores = 48\n",
    "start = time.time()\n",
    "evaluations = 1000\n",
    "pbar = tqdm(total=evaluations, desc=\"Hyperopt\")\n",
    "best_param = optimize(evals=evaluations,\n",
    "                      optimizer=tpe.suggest,\n",
    "                      trials=trials)\n",
    "print(\"------------------------------------\")\n",
    "print(\"The best hyperparameters are: \", \"\\n\")\n",
    "print(best_param)\n",
    "end = time.time()\n",
    "print('Time elapsed to optimize {0} executions: {1}'.format(evaluations, end - start))\n",
    "best_param['hidden_layer_sizes'] = hidden_layer_sizes_list[best_param['hidden_layer_sizes']]\n",
    "best_param['activation'] = activation_list[best_param['activation']]\n",
    "best_param['solver'] = solver_list[best_param['solver']]\n",
    "best_param['learning_rate'] = learning_rate_list[best_param['learning_rate']]\n",
    "best_param['batch_size'] = batch_size_list[best_param['batch_size']]\n",
    "best_param['max_iter'] = max_iter_list[best_param['max_iter']]\n",
    "print('\\n Best score:')\n",
    "score(best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best params and train the model\n",
    "mlp_opt = MLPClassifier(\n",
    "            hidden_layer_sizes = best_param['hidden_layer_sizes'],\n",
    "            activation = best_param['activation'],\n",
    "            solver = best_param['solver'],\n",
    "            alpha = best_param['alpha'],\n",
    "            learning_rate = best_param['learning_rate'],\n",
    "            batch_size = best_param['batch_size'],\n",
    "            max_iter = best_param['max_iter'],\n",
    "            shuffle = True,\n",
    "            random_state = 42,\n",
    "            warm_start = False,\n",
    "            early_stopping = True,\n",
    "            validation_fraction = 0.1,\n",
    "    )\n",
    "\n",
    "fitted_model = mlp_opt.fit(X=X_train,y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = fitted_model.predict_proba(X_test)\n",
    "y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = accuracy_score(y_test, fitted_model.predict(X_test))\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline \n",
    "x_list =range(len(train_acc_list))\n",
    "\n",
    "plt.plot(x_list, train_acc_list, label='Training accuracy')\n",
    "plt.plot(x_list, test_acc_list, label='Testing accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_list, train_loss_list, label='Training loss')\n",
    "plt.plot(x_list, test_loss_list, label='Testing loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
